{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd() #get working directory\n",
    "\n",
    "td_top5 = pd.read_csv(path +'/processed_data/historical_team_data_top_5.csv')\n",
    "td_top5_gs = pd.read_csv(path +'/processed_data/historical_team_data_top_5_gs.csv')\n",
    "td_top5_adv = pd.read_csv(path + '/processed_data/historical_team_data_top_5_advanced.csv')\n",
    "td_all = pd.read_csv(path +'/processed_data/historical_team_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop data with NULLs... something is wrong with Charlotte Bobcat/Hornets data\n",
    "td_top5 = td_top5.dropna(how='any') \n",
    "td_top5_gs = td_top5_gs.dropna(how='any')\n",
    "td_top5_adv = td_top5_adv.dropna(how='any')\n",
    "td_all = td_all.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove 2017 season from models because it is incomplete\n",
    "td_top5 = td_top5.loc[(td_top5['season'] != 2017)]\n",
    "td_top5_gs = td_top5_gs.loc[(td_top5_gs['season'] != 2017)]\n",
    "td_top5_adv = td_top5_adv.loc[(td_top5_adv['season'] != 2017)]\n",
    "td_all = td_all.loc[(td_all['season'] != 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 331\n",
      "Number of samples in validation data: 143\n",
      "Coefficients: [ -7.07321032e-03  -5.98505312e-03  -2.11597438e-02  -1.22495890e-02\n",
      "  -2.49508914e-02  -8.90342805e-03  -1.89302695e-02   1.54307287e-04\n",
      "  -1.05307532e-02  -7.51362554e-03   1.63618024e-02   5.95133426e-03\n",
      "   2.28618508e-02   2.38321365e-02  -1.98195601e-02  -7.55392572e-04\n",
      "   2.50615069e-02  -1.37338266e-03   3.43314660e-02   7.32090064e-02\n",
      "   4.35947833e-02  -2.70434813e-01  -7.99216197e-01   3.20691266e-02]\n",
      "Mean squared error: 0.007942745875591828\n",
      "R^2: 0.664485228015\n"
     ]
    }
   ],
   "source": [
    "#model training attempt 1 - linear regression\n",
    "##assumptions: \n",
    "##time series component will be ignored (assume league stats are same in 2016 compared to 2000) ... remove season\n",
    "##no normalization of stats - could be useful\n",
    "##will remove team name ... though it could be coded as a categorical variable since some orgs are better than others\n",
    "##will do with all players\n",
    "##drop w/l, will predict wp\n",
    "data = td_all.drop(['season', 'team_name', 'w', 'l'], axis=1)\n",
    "\n",
    "#define x, y for prediction\n",
    "x = data.drop(['wp'], axis=1)\n",
    "y = data['wp']\n",
    "\n",
    "# Split data into training and validation set  using sklearn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n",
    "#perform linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr = linear_model.LinearRegression() #create regression object\n",
    "regr.fit(x_train, y_train)\n",
    "y_pred = regr.predict(x_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_pred - y_train) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_train, y_pred))\n",
    "\n",
    "###Prediction is ok... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.009268811781261157\n",
      "R^2: 0.547555141713\n"
     ]
    }
   ],
   "source": [
    "#Test models against test set\n",
    "##All players model\n",
    "y_test_pred = regr.predict(x_test)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_test_pred - y_test) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "#Model is really mediocre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 331\n",
      "Number of samples in validation data: 143\n",
      "Coefficients: [ -8.88367954e-02  -8.40084161e-02  -4.73056663e-02   4.83227776e-02\n",
      "  -4.31502373e-02   7.39768478e-02  -6.43107657e-02  -2.30102464e-04\n",
      "  -1.44083928e-02  -5.31555808e-03   7.64141891e-03   1.10087105e-02\n",
      "   4.18921855e-02   4.16576717e-02  -4.69101733e-02  -1.02853961e-02\n",
      "   7.83910241e-02   7.62850384e-04   8.89900169e-01  -8.61672030e-02\n",
      "  -1.32828404e+00  -4.45236647e-01   1.26639390e+00   1.20284616e-02]\n",
      "Mean squared error: 0.010107860052115131\n",
      "R^2: 0.573027210771\n"
     ]
    }
   ],
   "source": [
    "#model training attempt 2 - linear regression\n",
    "##assumptions: \n",
    "##time series component will be ignored (assume league stats are same in 2016 compared to 2000) ... remove season\n",
    "##no normalization of stats - could be useful\n",
    "##will remove team name ... though it could be coded as a categorical variable since some orgs are better than others\n",
    "##will do with all players\n",
    "##drop w/l, will predict wp\n",
    "data = td_top5.drop(['season', 'team_name', 'w', 'l'], axis=1)\n",
    "\n",
    "#define x, y for prediction\n",
    "x = data.drop(['wp'], axis=1)\n",
    "y = data['wp']\n",
    "\n",
    "# Split data into training and validation set  using sklearn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n",
    "#perform linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr_top5 = linear_model.LinearRegression() #create regression object\n",
    "regr_top5.fit(x_train, y_train)\n",
    "y_pred = regr_top5.predict(x_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr_top5.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_pred - y_train) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_train, y_pred))\n",
    "\n",
    "#R^2 is worse but not an insurmountable delta (.68 vs .60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.01040608892909823\n",
      "R^2: 0.492040453301\n"
     ]
    }
   ],
   "source": [
    "#Test top 5 player model against test set\n",
    "y_test_pred = regr_top5.predict(x_test)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_test_pred - y_test) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "#Model definitely performs worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For now will continue on to building function. However, there is clear need to improve model. Ideas:\n",
    "#1. Improve features\n",
    "#2. Improve top 5 player selection process\n",
    "#3. Normalize data across years and/or normalize data per year to account for change in league.\n",
    "#4. Try additional model types, though I suspect feature development is more critical at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save models for use in app\n",
    "import pickle\n",
    "\n",
    "filename = 'all_players.sav'\n",
    "pickle.dump(regr, open(path + '/models/' + filename, 'wb'))\n",
    "\n",
    "filename = 'top_5_players.sav'\n",
    "pickle.dump(regr_top5, open(path + '/models/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 331\n",
      "Number of samples in validation data: 143\n",
      "Coefficients: [-1.60724094 -3.36780406 -0.64035353  1.67933158 -0.92108634  3.33154147\n",
      " -1.28907357 -0.00533107 -0.16690865 -0.11000674  0.22577361  0.2550526\n",
      "  0.23991588  0.32532658 -0.43961534 -0.09772759  3.87724494  0.04974511\n",
      "  0.20385777 -0.05018898 -0.30687578 -0.18515484  0.26579196  0.20744323]\n",
      "Mean squared error: 0.017124023933086926\n",
      "R^2: 0.573027210771\n"
     ]
    }
   ],
   "source": [
    "#Data normalization via 'feature scaling'\n",
    "data = td_top5.drop(['season', 'team_name', 'w', 'l'], axis=1)\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "#define x, y for prediction\n",
    "x = data.drop(['wp'], axis=1)\n",
    "y = data['wp']\n",
    "\n",
    "# Split data into training and validation set  using sklearn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n",
    "#perform linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr_top5 = linear_model.LinearRegression() #create regression object\n",
    "regr_top5.fit(x_train, y_train)\n",
    "y_pred = regr_top5.predict(x_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr_top5.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_pred - y_train) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_train, y_pred))\n",
    "\n",
    "#Apparently feature scaling does nothing to the prediction accuracy? Because everything is in the same proportion maybe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 331\n",
      "Number of samples in validation data: 143\n",
      "Coefficients: [-1.41806094 -2.81785166 -0.48677277  1.27200593 -0.74516263  2.59100996\n",
      " -1.07987182 -0.0049512  -0.14603141 -0.08982295  0.18510309  0.19714107\n",
      "  0.22375778  0.27885864 -0.3813266  -0.08426582  3.59341471  0.04167848\n",
      "  0.15320221 -0.03411591 -0.21182935 -0.12832989  0.21962328  0.18310914]\n",
      "Mean squared error: 0.4440924211656911\n",
      "R^2: 0.573027210771\n"
     ]
    }
   ],
   "source": [
    "#Data normalization via 'standard score'\n",
    "data = td_top5.drop(['season', 'team_name', 'w', 'l'], axis=1)\n",
    "data = (data - data.mean()) / data.std()\n",
    "\n",
    "#define x, y for prediction\n",
    "x = data.drop(['wp'], axis=1)\n",
    "y = data['wp']\n",
    "\n",
    "# Split data into training and validation set  using sklearn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n",
    "#perform linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr_top5 = linear_model.LinearRegression() #create regression object\n",
    "regr_top5.fit(x_train, y_train)\n",
    "y_pred = regr_top5.predict(x_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr_top5.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_pred - y_train) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_train, y_pred))\n",
    "\n",
    "#Apparently standard score doesn't do anything either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 331\n",
      "Number of samples in validation data: 143\n",
      "Coefficients: [  1.87206342e-01   3.21779921e-02  -8.13540014e-02   3.16474878e-02\n",
      "   8.17204029e-03   9.62884729e-03   3.58647587e-02  -5.65637087e-04\n",
      "   1.03926283e+00  -8.93437645e-01  -5.64834299e-02  -6.40872809e-01\n",
      "  -9.18239629e-03  -1.93886235e-02  -1.73138047e-02   2.67955822e-02\n",
      "   3.58231522e-02  -5.04604996e-02   7.45911232e-03]\n",
      "Mean squared error: 0.008950049872914695\n",
      "R^2: 0.621935034886\n"
     ]
    }
   ],
   "source": [
    "#Try substracting season averages for each team to see if this scaling helps prediction\n",
    "\n",
    "data = td_top5.drop(['team_name', 'w', 'l','wp'], axis=1)\n",
    "wp = td_top5['wp']\n",
    "season_average = pd.read_csv(path +'/processed_data/season_average_top_5_data.csv', index_col=0)\n",
    "\n",
    "new_data = pd.DataFrame()\n",
    "for i in season_average['season'].unique():\n",
    "    diff = data[data['season']==i] - season_average[season_average['season']==i].loc['mean']\n",
    "    new_data = new_data.append(diff)\n",
    "\n",
    "data = new_data.drop(['2p', '3p', 'fg','ft', 'mp', 'season'], axis=1)\n",
    "data = pd.concat([wp, data], axis=1)\n",
    "\n",
    "#define x, y for prediction\n",
    "x = data.drop(['wp'], axis=1)\n",
    "y = data['wp']\n",
    "\n",
    "# Split data into training and validation set  using sklearn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n",
    "#perform linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr_top5 = linear_model.LinearRegression() #create regression object\n",
    "regr_top5.fit(x_train, y_train)\n",
    "y_pred = regr_top5.predict(x_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr_top5.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_pred - y_train) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_train, y_pred))\n",
    "\n",
    "#R^2 improved by approximately .04 for training set... seems like a meaningful improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'top_5_players_sa.sav'\n",
    "pickle.dump(regr_top5, open(path + '/models/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.008896088065509726\n",
      "R^2: 0.565749159752\n"
     ]
    }
   ],
   "source": [
    "#Test top 5 player model against test set\n",
    "y_test_pred = regr_top5.predict(x_test)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_test_pred - y_test) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "#Model performs much better on test set (.09 R2 improvement despite for top 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 331\n",
      "Number of samples in validation data: 143\n",
      "Coefficients: [-0.54628527 -0.00655451 -0.09792054 -0.00764593  0.01105216  0.00817982\n",
      "  0.02905397  0.02015507  1.30914952 -0.38615621 -0.02177514 -0.47113393\n",
      " -0.00904341  0.0080602  -0.01331702  0.03049581  0.03361279 -0.05321272\n",
      " -0.01481774]\n",
      "Mean squared error: 0.008453978925293735\n",
      "R^2: 0.642889895269\n"
     ]
    }
   ],
   "source": [
    "#Try substracting season averages for each team to see if this scaling helps prediction\n",
    "\n",
    "data = td_top5_gs.drop(['team_name', 'w', 'l','wp'], axis=1)\n",
    "wp = td_top5_gs['wp']\n",
    "season_average = pd.read_csv(path +'/processed_data/season_average_top_5_gs_data.csv', index_col=0)\n",
    "\n",
    "new_data = pd.DataFrame()\n",
    "for i in season_average['season'].unique():\n",
    "    diff = data[data['season']==i] - season_average[season_average['season']==i].loc['mean']\n",
    "    new_data = new_data.append(diff)\n",
    "\n",
    "data = new_data.drop(['2p', '3p', 'fg','ft', 'mp', 'season'], axis=1)\n",
    "data = pd.concat([wp, data], axis=1)\n",
    "\n",
    "#define x, y for prediction\n",
    "x = data.drop(['wp'], axis=1)\n",
    "y = data['wp']\n",
    "\n",
    "# Split data into training and validation set  using sklearn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n",
    "#perform linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr_top5 = linear_model.LinearRegression() #create regression object\n",
    "regr_top5.fit(x_train, y_train)\n",
    "y_pred = regr_top5.predict(x_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr_top5.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_pred - y_train) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_train, y_pred))\n",
    "\n",
    "#R^2 is slightly worse in this case versus ranking by minutes played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'top_5_players_sa_gs.sav'\n",
    "pickle.dump(regr_top5, open(path + '/models/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.010285639592949489\n",
      "R^2: 0.49792002925\n"
     ]
    }
   ],
   "source": [
    "#Test top 5 player model against test set\n",
    "y_test_pred = regr_top5.predict(x_test)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_test_pred - y_test) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "#This model performed best against the test set... I really should be doing a cross validation method\n",
    "#right now I'm definitely introducing bias by running against the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 331\n",
      "Number of samples in validation data: 143\n",
      "Coefficients: [ -1.00644066e-01   1.44007823e-03   3.16650076e-02  -1.88751780e-03\n",
      "   2.10832694e-01   4.76553132e-03   3.07169218e-02  -2.57903738e-02\n",
      "   3.86834577e-02  -2.76608277e-02   7.51033364e-02  -7.66928798e-02\n",
      "   3.57014695e-02  -2.60734466e-02   3.39781956e-02  -1.21090156e-01\n",
      "   7.41353448e-01   7.61356902e-03  -4.21758456e-01   2.58257739e-03\n",
      "  -1.95060150e-01  -7.83573816e-02   6.00394241e-02  -6.55250698e-02\n",
      "   1.01402387e-01  -8.80673144e-02  -1.46876517e-02  -1.02460412e-02\n",
      "  -2.23236559e-02   1.14840030e-01   3.00175067e-04   5.51535323e-03\n",
      "  -4.34888240e-02   7.95504922e-02   5.16930169e-02   9.10908094e-03\n",
      "  -6.88011514e-02   6.46249642e+00]\n",
      "Mean squared error: 0.002095800947333057\n",
      "R^2: 0.911469888628\n"
     ]
    }
   ],
   "source": [
    "#Advanced stats experimentation + season scaling\n",
    "\n",
    "#data = td_top5_adv.drop(['team_name', 'w', 'l'], axis=1)\n",
    "\n",
    "data = td_top5_adv.drop(['team_name', 'w', 'l','wp'], axis=1)\n",
    "wp = td_top5_adv['wp']\n",
    "season_average = pd.read_csv(path +'/processed_data/season_average_top_5_advanced_data.csv', index_col=0)\n",
    "\n",
    "new_data = pd.DataFrame()\n",
    "for i in season_average['season'].unique():\n",
    "    diff = data[data['season']==i] - season_average[season_average['season']==i].loc['mean']\n",
    "    new_data = new_data.append(diff)\n",
    "\n",
    "data = new_data.drop(['2p', '3p', 'fg','ft', 'mp', 'season'], axis=1)\n",
    "data = pd.concat([wp, data], axis=1)\n",
    "\n",
    "#define x, y for prediction\n",
    "x = data.drop(['wp'], axis=1)\n",
    "y = data['wp']\n",
    "\n",
    "# Split data into training and validation set  using sklearn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n",
    "#perform linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr_top5 = linear_model.LinearRegression() #create regression object\n",
    "regr_top5.fit(x_train, y_train)\n",
    "y_pred = regr_top5.predict(x_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr_top5.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_pred - y_train) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_train, y_pred))\n",
    "\n",
    "#R^2 is slightly worse in this case versus ranking by minutes played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'top_5_players_sa_advanced.sav'\n",
    "pickle.dump(regr_top5, open(path + '/models/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.002600941299466077\n",
      "R^2: 0.873038470796\n"
     ]
    }
   ],
   "source": [
    "#Test top 5 player model against test set\n",
    "y_test_pred = regr_top5.predict(x_test)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_test_pred - y_test) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "#This model performed best against the test set... I really should be doing a cross validation method\n",
    "#right now I'm definitely introducing bias by running against the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 331\n",
      "Number of samples in validation data: 143\n",
      "Coefficients: [ -7.67185762e-05  -5.18520499e-02   7.68148358e-03   1.38470585e-02\n",
      "  -1.86026971e-02  -2.02358932e-02   1.09625653e-02  -2.45033901e-02\n",
      "   3.61900583e-04   5.92084485e-02   3.46704052e-02  -4.61232869e-02\n",
      "   2.85001068e-02  -2.81579049e-02   4.08123229e-02   5.02819896e-03\n",
      "  -1.07112208e-02   2.02245802e-02  -1.58911104e-03  -3.70035955e-01\n",
      "   3.97806680e-02   7.74862756e+00  -4.70243806e-01  -6.18644405e+00\n",
      "   4.76514672e-03  -6.66126470e-02   1.58327833e+00  -1.27389561e-01\n",
      "  -7.13224053e-02  -3.43536486e-02   9.80200944e-02  -2.30888172e-02\n",
      "   9.90775981e-02  -5.10355637e-02   6.02556220e-04   3.36100065e-02\n",
      "   4.88765327e-02  -9.84751914e-03  -1.56825855e-02   5.53914976e+00\n",
      "  -7.46765755e-02  -4.79053427e-02   6.05317071e-02   6.88370103e-03]\n",
      "Mean squared error: 0.0020560168494829167\n",
      "R^2: 0.913150434969\n"
     ]
    }
   ],
   "source": [
    "#Advanced stats experimentation + w/o season scaling\n",
    "\n",
    "data = td_top5_adv.drop(['team_name', 'w', 'l'], axis=1)\n",
    "\n",
    "#define x, y for prediction\n",
    "x = data.drop(['wp'], axis=1)\n",
    "y = data['wp']\n",
    "\n",
    "# Split data into training and validation set  using sklearn function\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n",
    "#perform linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr_top5 = linear_model.LinearRegression() #create regression object\n",
    "regr_top5.fit(x_train, y_train)\n",
    "y_pred = regr_top5.predict(x_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr_top5.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_pred - y_train) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_train, y_pred))\n",
    "\n",
    "#R^2 is slightly worse in this case versus ranking by minutes played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'top_5_players_advanced.sav'\n",
    "pickle.dump(regr_top5, open(path + '/models/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.0029499163151636347\n",
      "R^2: 0.856003714319\n"
     ]
    }
   ],
   "source": [
    "#Test top 5 player model against test set\n",
    "y_test_pred = regr_top5.predict(x_test)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", np.mean((y_test_pred - y_test) ** 2))\n",
    "#The R^2\n",
    "print(\"R^2:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "#This model performed best against the test set... I really should be doing a cross validation method\n",
    "#right now I'm definitely introducing bias by running against the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
